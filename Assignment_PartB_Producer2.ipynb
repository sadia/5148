{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import random\n",
    "import csv\n",
    "import datetime as dt\n",
    "import pymongo\n",
    "import geohash2\n",
    "import time\n",
    "\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.fit5148_assignment_db\n",
    "climate_fire = db.climate_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AQUA_data = []\n",
    "\n",
    "# Open the climate_historic CSV file \n",
    "with open('fire_AQUA_streaming.csv', 'r', encoding='utf-8-sig') as streaming_file: \n",
    "    # Skip header line\n",
    "    next(streaming_file)\n",
    "    \n",
    "  # Read the fire_historic CSV file, append each row to the array\n",
    "    for row in csv.reader(streaming_file):\n",
    "        AQUA_data.append(row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    topic = 'AQUA' \n",
    "    \n",
    "    print('Publishing records..')\n",
    "    producer = connect_kafka_producer()\n",
    "    \n",
    "    for i in range(len(AQUA_data)):\n",
    "        values = random.choice(AQUA_data)\n",
    "        geo3 = str(geohash2.encode(float(values[0]), float(values[1]), precision=3))\n",
    "        data = topic + ', ' + dt.datetime.now().strftime(\"%X\") + ', ' + str(values[0]) + ', ' + str(values[1]) + ', ' + str(values[2]) + ', ' + str(values[3]) + ', ' + str(values[4])\n",
    "        \n",
    "        publish_message(producer, topic, geo3, data)\n",
    "        sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
